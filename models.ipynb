{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Import Libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scjpnlib.utils.file_io import FileManager\n",
    "import scjpnlib.utils as scjpnutils\n",
    "from IPython.core.display import HTML, Markdown\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "import dask_ml.model_selection as dcv\n",
    "from dask.distributed import Client\n",
    "import joblib\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier \n",
    "# import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "K = 3 # num folds for cross-val\n",
    "n_jobs = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Configs for this Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'SEED': 42,\n 'DecisionTreeClassifier': {'trials': {'run': False,\n   'array': [{'gridsearch': {'run': False,\n      'param_grid': {'criterion': ['entropy', 'gini'],\n       'splitter': ['best'],\n       'max_depth': [10, 50, 75, None],\n       'min_samples_split': [2],\n       'max_features': ['auto', 'sqrt', 'log2']},\n      'last_best': {'criterion': 'entropy',\n       'splitter': 'best',\n       'max_depth': None,\n       'min_samples_split': 2,\n       'max_features': 'sqrt'}}}]},\n  'params': {'criterion': 'entropy',\n   'splitter': 'best',\n   'max_depth': None,\n   'min_samples_split': 2,\n   'max_features': 'sqrt'}},\n 'RandomForestClassifier': {'trials': {'run': False,\n   'array': [{'gridsearch': {'run': False,\n      'param_grid': {'bootstrap': [True, False],\n       'criterion': ['entropy', 'gini'],\n       'max_features': ['auto', 'sqrt', 'log2'],\n       'max_depth': [10, 50, 75, None],\n       'n_estimators': [100, 500, 1000]},\n      'last_best': {'bootstrap': True,\n       'criterion': 'entropy',\n       'max_depth': 75,\n       'max_features': 'auto',\n       'n_estimators': 1000}}}]},\n  'params': {'bootstrap': True,\n   'criterion': 'entropy',\n   'max_depth': 75,\n   'max_features': 'auto',\n   'n_estimators': 1000}},\n 'XGBClassifier': {'trials': {'run': False,\n   'array': [{'gridsearch': {'run': False,\n      'param_grid': {'learning_rate': [0.1],\n       'max_depth': [3],\n       'min_child_weight': [1],\n       'subsample': [1],\n       'gamma': [0, 1, 2, 5, 10],\n       'n_estimators': [100]},\n      'last_best': {'learning_rate': 0.1,\n       'max_depth': 3,\n       'min_child_weight': 1,\n       'subsample': 1,\n       'gamma': 1,\n       'n_estimators': 100}}},\n    {'gridsearch': {'run': False,\n      'param_grid': {'learning_rate': [0.1, 0.3, 0.6, 1.0],\n       'max_depth': [3],\n       'min_child_weight': [1],\n       'subsample': [1],\n       'gamma': [1],\n       'n_estimators': [100]},\n      'last_best': {'learning_rate': 0.3,\n       'max_depth': 3,\n       'min_child_weight': 1,\n       'subsample': 1,\n       'gamma': 1,\n       'n_estimators': 100}}},\n    {'gridsearch': {'run': False,\n      'param_grid': {'learning_rate': [0.3],\n       'max_depth': [3, 5, 10, 25, 50],\n       'min_child_weight': [1],\n       'subsample': [1],\n       'gamma': [1],\n       'n_estimators': [100]},\n      'last_best': {'learning_rate': 0.3,\n       'max_depth': 10,\n       'min_child_weight': 1,\n       'subsample': 1,\n       'gamma': 1,\n       'n_estimators': 100}}},\n    {'gridsearch': {'run': False,\n      'param_grid': {'learning_rate': [0.3],\n       'max_depth': [10],\n       'min_child_weight': [1, 3, 5],\n       'subsample': [1],\n       'gamma': [1],\n       'n_estimators': [100]},\n      'last_best': {'learning_rate': 0.3,\n       'max_depth': 10,\n       'min_child_weight': 1,\n       'subsample': 1,\n       'gamma': 1,\n       'n_estimators': 100}}},\n    {'gridsearch': {'run': False,\n      'param_grid': {'learning_rate': [0.3],\n       'max_depth': [10],\n       'min_child_weight': [1],\n       'subsample': [0.1, 0.25, 0.5, 1.0],\n       'gamma': [1],\n       'n_estimators': [100]},\n      'last_best': {'learning_rate': 0.3,\n       'max_depth': 10,\n       'min_child_weight': 1,\n       'subsample': 1,\n       'gamma': 1,\n       'n_estimators': 100}}},\n    {'gridsearch': {'run': False,\n      'param_grid': {'learning_rate': [0.3],\n       'max_depth': [10],\n       'min_child_weight': [1],\n       'subsample': [1.0],\n       'gamma': [1],\n       'n_estimators': [100, 500, 750, 1000]},\n      'last_best': {'learning_rate': 0.3,\n       'max_depth': 10,\n       'min_child_weight': 1,\n       'subsample': 1,\n       'gamma': 1,\n       'n_estimators': 100}}}]},\n  'params': {'learning_rate': 0.3,\n   'max_depth': 10,\n   'min_child_weight': 1,\n   'subsample': 1,\n   'gamma': 1,\n   'n_estimators': 100}}}"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "fm = FileManager()\n",
    "\n",
    "models_config = fm.load_json('models-config.txt')\n",
    "models_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_data_cached = 'data_cached' in models_config\n",
    "data_config = models_config['data_cached'] if is_data_cached else fm.load_json('eda-config.txt')\n",
    "digest = data_config['digest'] if is_data_cached else scjpnutils.json_to_md5_hash_digest(data_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "EDA description: (digest: afdac7327a7b30faeede4a7e88650a6e): 0.10 test_ratio, flat insig cat hat handling with threshold 10\n"
    }
   ],
   "source": [
    "print(f\"EDA description: {'CACHED ' if is_data_cached else ''}(digest: {digest}): {data_config['eda_desc']['short']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><br>\n",
    "<h2>Load TEST/TRAIN Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = models_config['SEED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loaded wrangled-labeled-data-train-afdac7327a7b30faeede4a7e88650a6e.csv\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 53460 entries, 44928 to 56422\nColumns: 107 entries, installer_distr to pump_age\ndtypes: float64(107)\nmemory usage: 44.0 MB\n"
    }
   ],
   "source": [
    "fname = scjpnutils.get_data_fname(data_config, data_kwargs={'is_labels':False,'type':'train','is_cached':is_data_cached})\n",
    "data_train = pd.read_csv(fname, index_col=0)\n",
    "print(f\"loaded {fname}\\n\")\n",
    "data_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loaded labels-train-afdac7327a7b30faeede4a7e88650a6e.csv\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 53460 entries, 44928 to 56422\nData columns (total 1 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   status_group  53460 non-null  object\ndtypes: object(1)\nmemory usage: 835.3+ KB\n"
    }
   ],
   "source": [
    "fname = scjpnutils.get_data_fname(data_config, data_kwargs={'is_labels':True,'type':'train','is_cached':is_data_cached})\n",
    "y_train = pd.read_csv(fname, index_col=0)\n",
    "print(f\"loaded {fname}\\n\")\n",
    "y_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = y_train.status_group.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loaded wrangled-labeled-data-test-afdac7327a7b30faeede4a7e88650a6e.csv\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 5940 entries, 2980 to 26085\nColumns: 107 entries, installer_distr to pump_age\ndtypes: float64(107)\nmemory usage: 4.9 MB\n"
    }
   ],
   "source": [
    "fname = scjpnutils.get_data_fname(data_config, data_kwargs={'is_labels':False,'type':'test','is_cached':is_data_cached})\n",
    "data_test = pd.read_csv(fname, index_col=0)\n",
    "print(f\"loaded {fname}\\n\")\n",
    "data_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loaded labels-test-afdac7327a7b30faeede4a7e88650a6e.csv\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 5940 entries, 2980 to 26085\nData columns (total 1 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   status_group  5940 non-null   object\ndtypes: object(1)\nmemory usage: 92.8+ KB\n"
    }
   ],
   "source": [
    "fname = scjpnutils.get_data_fname(data_config, data_kwargs={'is_labels':True,'type':'test','is_cached':is_data_cached})\n",
    "y_test = pd.read_csv(fname, index_col=0)\n",
    "print(f\"loaded {fname}\\n\")\n",
    "y_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><br>\n",
    "<h2>Models</h2>\n",
    "\n",
    "<h3>General functions for building Classifiers and running trials</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gs_find_best_params(clf, param_grid):\n",
    "    display(HTML(f\"param_grid for {type(clf)} GridSearch:<br><pre>{params}</pre>\"))\n",
    "    grid_clf = GridSearchCV(\n",
    "        clf, \n",
    "        param_grid, \n",
    "        cv=K, \n",
    "        n_jobs=-1\n",
    "        , verbose=20\n",
    "    )\n",
    "    # with joblib.parallel_backend('dask'):\n",
    "    #     %time _ = grid_clf.fit(data_train, y_train)\n",
    "    %time _ = grid_clf.fit(data_train, y_train)\n",
    "    return grid_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_fit(clf, data_train, y_train):\n",
    "    # with joblib.parallel_backend('dask'):\n",
    "    #     %time clf.fit(data_train, y_train)\n",
    "    %time clf.fit(data_train, y_train)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_preds(X, y, preds, dataset_name, classes):\n",
    "    display(HTML(\"<p><br>\"))\n",
    "    _accuracy = accuracy_score(y, preds)\n",
    "    display(HTML(f\"{dataset_name} Accuracy: {round(_accuracy*100,4)}\"))\n",
    "    display(HTML(\"<p><br>\"))\n",
    "    display(HTML(f\"<pre>{classification_report(y, preds, target_names=classes)}</pre>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_run_trial(clf, params, run_trials_gridsearch):\n",
    "    best_parameters = gs_find_best_params(clf, params) if run_trials_gridsearch else params\n",
    "\n",
    "    display(HTML(\"<p><br>\"))\n",
    "    display(HTML(f\"Grid Search {'(previously) ' if not run_trials_gridsearch else ''}found the following optimal parameters: \"))\n",
    "    s_best_params = \"\"\n",
    "    for param_name in list(best_parameters.keys()):\n",
    "        s_best_params += f\"\\t{param_name}: {best_parameters[param_name]}\\n\"\n",
    "    display(HTML(f\"<pre>{s_best_params}</pre>\"))\n",
    "\n",
    "    display(HTML(\"<p><br>\"))\n",
    "    display(HTML(\"Fitting classifer...\"))\n",
    "    clf = clf.set_params(**best_parameters)\n",
    "    clf = clf_fit(clf, data_train, y_train)\n",
    "    s_all_done = \"\\tALL DONE!\"\n",
    "    display(HTML(f\"<pre>{s_all_done}</pre>\"))\n",
    "\n",
    "    display(HTML(\"<p><br>\"))\n",
    "    display(HTML(\"Predicting labels on training data...\"))\n",
    "    pred_train = clf.predict(data_train)\n",
    "    display(HTML(f\"<pre>{s_all_done}</pre>\"))\n",
    "    summarize_preds(data_train, y_train, pred_train, 'Training', classes)\n",
    "\n",
    "    # display(HTML(\"<p><br>\"))\n",
    "    # display(HTML(\"Computing cross-val score on training data...\"))\n",
    "    # cv_score_train = cross_val_score(clf, data_train, y_train, cv=K)\n",
    "    # display(HTML(f\"<pre>{s_all_done}</pre>\"))\n",
    "    # display(HTML(f\"cross_val_score: {np.mean(cv_score_train)}\"))\n",
    "\n",
    "    display(HTML(\"<p><br>\"))\n",
    "    display(HTML(f\"Predicting labels on testing data...\"))\n",
    "    pred_test = clf.predict(data_test)\n",
    "    display(HTML(f\"<pre>{s_all_done}</pre>\"))\n",
    "    summarize_preds(data_test, y_test, pred_test, 'Testing', classes)\n",
    "    \n",
    "    # display(HTML(\"<p><br>\"))\n",
    "    # display(HTML(f\"Computing cross-val score on testing data...\"))\n",
    "    # cv_score_test = cross_val_score(clf, data_test, y_test, cv=K)\n",
    "    # display(HTML(f\"<pre>{s_all_done}</pre>\"))\n",
    "    # display(HTML(f\"cross_val_score: {np.mean(cv_score_test)}\"))\n",
    "\n",
    "    return clf, best_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><br>\n",
    "<h2>Initialize Dask-Client (to Dask backend for parallelization) <i>(DISABLED for now)</i></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # local\n",
    "# # dask_client = Client(n_workers=2, threads_per_worker=8, memory_limit='8GB') #spawns a local cluster; memory_limit is per worker\n",
    "# dask_client = Client(n_workers=1, threads_per_worker=n_jobs, memory_limit='16GB') #spawns a local cluster; memory_limit is per worker\n",
    "\n",
    "# # for Kubernetes dask scheduler/worker cluster in GCP - but this costs money to run the cluster AND requires a lot more work for data parallelization!\n",
    "# # scheduler_address = '35.230.13.87'\n",
    "# # dask_client = Client(f'tcp://{scheduler_address}:8786')\n",
    "\n",
    "# dask_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><br>\n",
    "<h3>Decision Tree Classifier</h3>\n",
    "<h4>Trials</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "models_config['DecisionTreeClassifier']['trials']['run']: False"
     },
     "metadata": {}
    }
   ],
   "source": [
    "trials = models_config['DecisionTreeClassifier']['trials']\n",
    "\n",
    "display(HTML(f\"models_config['DecisionTreeClassifier']['trials']['run']: {trials['run']}\"))\n",
    "if trials['run']:\n",
    "    trials_list = trials['array']\n",
    "\n",
    "    for i, trial in enumerate(trials_list):\n",
    "        display(HTML(f\"trial[{i}]['gridsearch']['run']: {trial['gridsearch']['run']}\"))\n",
    "        params = trial['gridsearch']['last_best'] if not trial['gridsearch']['run'] else trial['gridsearch']['param_grid']\n",
    "        rfclf, best_parameters = clf_run_trial(DecisionTreeClassifier(), params, trial['gridsearch']['run']) # note that best_parameters will be set to those used in the last trial\n",
    "\n",
    "else:\n",
    "    best_parameters = models_config['DecisionTreeClassifier']['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><br>\n",
    "<h4>Build Final Model with best params</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<p><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Grid Search (previously) found the following optimal parameters: "
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<pre>\tcriterion: entropy\n\tsplitter: best\n\tmax_depth: None\n\tmin_samples_split: 2\n\tmax_features: sqrt\n</pre>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<p><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Fitting classifer..."
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CPU times: user 243 ms, sys: 14.1 ms, total: 257 ms\nWall time: 258 ms\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<pre>\tALL DONE!</pre>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<p><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Predicting labels on training data..."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<pre>\tALL DONE!</pre>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<p><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Training Accuracy: 96.2608"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<p><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<pre>                         precision    recall  f1-score   support\n\n             functional       0.95      0.99      0.97     29062\n         non functional       0.93      0.86      0.89      3892\nfunctional needs repair       0.98      0.95      0.96     20506\n\n               accuracy                           0.96     53460\n              macro avg       0.95      0.93      0.94     53460\n           weighted avg       0.96      0.96      0.96     53460\n</pre>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<p><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Predicting labels on testing data..."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<pre>\tALL DONE!</pre>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<p><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Testing Accuracy: 72.6263"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<p><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<pre>                         precision    recall  f1-score   support\n\n             functional       0.77      0.78      0.77      3197\n         non functional       0.31      0.31      0.31       425\nfunctional needs repair       0.74      0.73      0.73      2318\n\n               accuracy                           0.73      5940\n              macro avg       0.61      0.61      0.61      5940\n           weighted avg       0.73      0.73      0.73      5940\n</pre>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "_, dtclf = clf_run_trial(DecisionTreeClassifier(), best_parameters, run_trials_gridsearch=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><br>\n",
    "<h3>Random Forest Classifier</h3>\n",
    "<h4>Trials</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "models_config['RandomForestClassifier']['trials']['run']: False"
     },
     "metadata": {}
    }
   ],
   "source": [
    "trials = models_config['RandomForestClassifier']['trials']\n",
    "\n",
    "display(HTML(f\"models_config['RandomForestClassifier']['trials']['run']: {trials['run']}\"))\n",
    "if trials['run']:\n",
    "    trials_list = trials['array']\n",
    "\n",
    "    for i, trial in enumerate(trials_list):\n",
    "        display(HTML(f\"trial[{i}]['gridsearch']['run']: {trial['gridsearch']['run']}\"))\n",
    "        params = trial['gridsearch']['last_best'] if not trial['gridsearch']['run'] else trial['gridsearch']['param_grid']\n",
    "        if trial['gridsearch']['run']:\n",
    "            params.update({'n_jobs': [-1]})\n",
    "        else:\n",
    "            params.update({'n_jobs':-1})\n",
    "        rfclf, best_parameters = clf_run_trial(RandomForestClassifier(), params, trial['gridsearch']['run']) # note that best_parameters will be set to those used in the last trial\n",
    "\n",
    "else:\n",
    "    best_parameters = models_config['RandomForestClassifier']['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><br>\n",
    "<h4>Build Final Model with best params</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<p><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Grid Search (previously) found the following optimal parameters: "
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<pre>\tbootstrap: True\n\tcriterion: entropy\n\tmax_depth: 75\n\tmax_features: auto\n\tn_estimators: 1000\n\tn_jobs: -1\n\tverbose: 1\n</pre>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<p><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Fitting classifer..."
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.1s\n[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   11.7s\n[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   24.6s\n[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   38.7s\nCPU times: user 4min 24s, sys: 5.91 s, total: 4min 30s\nWall time: 48.3 s\n[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   47.6s finished\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<pre>\tALL DONE!</pre>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<p><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Predicting labels on training data..."
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    1.4s\n[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    3.5s\n[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    5.7s\n[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    7.0s finished\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<pre>\tALL DONE!</pre>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<p><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Training Accuracy: 96.2608"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<p><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<pre>                         precision    recall  f1-score   support\n\n             functional       0.96      0.98      0.97     29062\n         non functional       0.92      0.87      0.89      3892\nfunctional needs repair       0.97      0.96      0.96     20506\n\n               accuracy                           0.96     53460\n              macro avg       0.95      0.94      0.94     53460\n           weighted avg       0.96      0.96      0.96     53460\n</pre>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<p><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Predicting labels on testing data..."
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.9s\n[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    1.1s finished\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<pre>\tALL DONE!</pre>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<p><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Testing Accuracy: 78.1818"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<p><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<pre>                         precision    recall  f1-score   support\n\n             functional       0.79      0.86      0.82      3197\n         non functional       0.46      0.26      0.34       425\nfunctional needs repair       0.81      0.76      0.79      2318\n\n               accuracy                           0.78      5940\n              macro avg       0.69      0.63      0.65      5940\n           weighted avg       0.77      0.78      0.77      5940\n</pre>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "best_parameters.update({'n_jobs':-1, 'verbose':1})\n",
    "rfclf, _ = clf_run_trial(RandomForestClassifier(), best_parameters, run_trials_gridsearch=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><br><br><br>\n",
    "<h3>XGBClassifier</h3>\n",
    "<h4>Trials</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "models_config['XGBClassifier']['trials']['run']: False"
     },
     "metadata": {}
    }
   ],
   "source": [
    "trials = models_config['XGBClassifier']['trials']\n",
    "\n",
    "display(HTML(f\"models_config['XGBClassifier']['trials']['run']: {trials['run']}\"))\n",
    "if trials['run']:\n",
    "    trials_list = trials['array']\n",
    "\n",
    "    for i, trial in enumerate(trials_list):\n",
    "        display(HTML(f\"trial[{i}]['gridsearch']['run']: {trial['gridsearch']['run']}\"))\n",
    "        params = trial['gridsearch']['last_best'] if not trial['gridsearch']['run'] else trial['gridsearch']['param_grid']\n",
    "        if trial['gridsearch']['run']:\n",
    "            params.update({'n_jobs': [-1]})\n",
    "        else:\n",
    "            params.update({'n_jobs':-1})\n",
    "        xgbclf, best_parameters = clf_run_trial(XGBClassifier(), params, trial['gridsearch']['run']) # note that best_parameters will be set to those used in the last trial\n",
    "\n",
    "else:\n",
    "    best_parameters = models_config['RandomForestClassifier']['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><br>\n",
    "<h4>Build Final Model with best params</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<p><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Grid Search (previously) found the following optimal parameters: "
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<pre>\tbootstrap: True\n\tcriterion: entropy\n\tmax_depth: 75\n\tmax_features: auto\n\tn_estimators: 1000\n\tn_jobs: -1\n\tverbose: 1\n\tverbosity: 1\n</pre>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<p><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Fitting classifer..."
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CPU times: user 5h 5min 45s, sys: 2min 26s, total: 5h 8min 12s\nWall time: 51min 12s\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<pre>\tALL DONE!</pre>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<p><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Predicting labels on training data..."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<pre>\tALL DONE!</pre>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<p><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Training Accuracy: 96.2608"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<p><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<pre>                         precision    recall  f1-score   support\n\n             functional       0.96      0.98      0.97     29062\n         non functional       0.92      0.87      0.89      3892\nfunctional needs repair       0.97      0.96      0.96     20506\n\n               accuracy                           0.96     53460\n              macro avg       0.95      0.94      0.94     53460\n           weighted avg       0.96      0.96      0.96     53460\n</pre>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<p><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Predicting labels on testing data..."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<pre>\tALL DONE!</pre>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<p><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Testing Accuracy: 77.3569"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<p><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<pre>                         precision    recall  f1-score   support\n\n             functional       0.78      0.85      0.82      3197\n         non functional       0.48      0.19      0.27       425\nfunctional needs repair       0.78      0.78      0.78      2318\n\n               accuracy                           0.77      5940\n              macro avg       0.68      0.60      0.62      5940\n           weighted avg       0.76      0.77      0.76      5940\n</pre>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "best_parameters.update({'n_jobs':-1, 'verbosity':1})\n",
    "xgbclf, _ = clf_run_trial(XGBClassifier(), best_parameters, run_trials_gridsearch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Voting Classifier with hard voting \n",
    "# vot_hard = VotingClassifier(estimators=voting_classifers, voting ='hard') \n",
    "# vot_hard.fit(data_train, y_train)  \n",
    "# pred_test = vot_hard.predict(data_test)\n",
    "\n",
    "# # using accuracy_score metric to predict accuracy \n",
    "# score = accuracy_score(y_test, pred_test) \n",
    "# print(\"Hard Voting Score % d\" % score) \n",
    "  \n",
    "# # Voting Classifier with soft voting \n",
    "# vot_soft = VotingClassifier(estimators=voting_classifers, voting ='soft') \n",
    "# vot_soft.fit(data_train, y_train)  \n",
    "# pred_test = vot_soft.predict(data_test)\n",
    "  \n",
    "# # using accuracy_score \n",
    "# score = accuracy_score(y_test, pred_test) \n",
    "# print(\"Soft Voting Score % d\" % score) "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitlearnenvconda70f0b4d48d474c1cba77a4611cdb5f3c",
   "display_name": "Python 3.7.6 64-bit ('learn-env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}