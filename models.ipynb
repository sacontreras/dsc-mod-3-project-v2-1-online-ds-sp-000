{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Import Libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "import dask_ml.model_selection as dcv\n",
    "from dask.distributed import Client\n",
    "import joblib\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><br>\n",
    "<h2>Load TEST/TRAIN Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 47520 entries, 3607 to 56422\nColumns: 1017 entries, installer_crety to construction_year\ndtypes: float64(1017)\nmemory usage: 369.1 MB\n"
    }
   ],
   "source": [
    "data_train = pd.read_csv('data-train-final.csv', index_col=0)\n",
    "data_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 47520 entries, 3607 to 56422\nData columns (total 1 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   status_group  47520 non-null  object\ndtypes: object(1)\nmemory usage: 742.5+ KB\n"
    }
   ],
   "source": [
    "y_train = pd.read_csv('labels-train-final.csv', index_col=0)\n",
    "y_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = y_train.status_group.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 11880 entries, 2980 to 41264\nColumns: 1017 entries, installer_crety to construction_year\ndtypes: float64(1017)\nmemory usage: 92.3 MB\n"
    }
   ],
   "source": [
    "data_test = pd.read_csv('data-test-final.csv', index_col=0)\n",
    "data_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 11880 entries, 2980 to 41264\nData columns (total 1 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   status_group  11880 non-null  object\ndtypes: object(1)\nmemory usage: 185.6+ KB\n"
    }
   ],
   "source": [
    "y_test = pd.read_csv('labels-test-final.csv', index_col=0)\n",
    "y_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><br>\n",
    "<h2>Models</h2>\n",
    "\n",
    "<h3>Random Forest Classifier</h3>\n",
    "<h4>Build/Train</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV hyperparameters TUNING\n",
    "param_grid = {\n",
    "  'bootstrap': [True, False],\n",
    "  'criterion': ['entropy', 'gini'],\n",
    "  'max_features': ['auto', 'sqrt', 'log2'],\n",
    "  'max_depth': [50, 100, None],\n",
    "  'n_estimators': [100, 500, 1000], # 100 is default\n",
    "}\n",
    "\n",
    "K = 3\n",
    "\n",
    "clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<Client: 'tcp://127.0.0.1:51851' processes=2 threads=16, memory=16.00 GB>",
      "text/html": "<table style=\"border: 2px solid white;\">\n<tr>\n<td style=\"vertical-align: top; border: 0px solid white\">\n<h3 style=\"text-align: left;\">Client</h3>\n<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n  <li><b>Scheduler: </b>tcp://127.0.0.1:51851</li>\n  <li><b>Dashboard: </b><a href='http://127.0.0.1:51850/status' target='_blank'>http://127.0.0.1:51850/status</a></li>\n</ul>\n</td>\n<td style=\"vertical-align: top; border: 0px solid white\">\n<h3 style=\"text-align: left;\">Cluster</h3>\n<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n  <li><b>Workers: </b>2</li>\n  <li><b>Cores: </b>16</li>\n  <li><b>Memory: </b>16.00 GB</li>\n</ul>\n</td>\n</tr>\n</table>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "dask_client = Client(n_workers=2, threads_per_worker=8, memory_limit='8GB') #spawns a local cluster; memory_limit is per worker\n",
    "dask_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dask Grid Search\n",
    "# grid_clf = dcv.GridSearchCV(\n",
    "#     clf, \n",
    "#     param_grid,\n",
    "#     cv=K\n",
    "# )\n",
    "# with joblib.parallel_backend('dask', scatter=[data_train, y_train]):\n",
    "#     %time _ = grid_clf.fit(data_train, y_train)\n",
    "#     best_parameters = grid_clf.best_params_\n",
    "\n",
    "# with dask backend, sklearn GridSearch CV appears to perform \"better\" (faster) than dask's GridSearchCV; dask's GridSearchCV also appears to use more memory and is sensitive to the local cluster's workers' memory_limit setting\n",
    "# # COMMENT OUT EVERYTHING BELOW TO GO WITH LAST GridSearchCV RESULT\n",
    "# grid_clf = GridSearchCV(\n",
    "#     clf, \n",
    "#     param_grid, \n",
    "#     cv=K, \n",
    "#     n_jobs=-1, \n",
    "#     verbose=20\n",
    "# )\n",
    "# with joblib.parallel_backend('dask', scatter=[data_train, y_train]):\n",
    "#     %time _ = grid_clf.fit(data_train, y_train)\n",
    "#     best_parameters = grid_clf.best_params_\n",
    "# # COMMENT OUT EVERYTHING ABOVE TO GO WITH LAST GridSearchCV RESULT\n",
    "\n",
    "# last run:\n",
    "# Grid Search found the following optimal parameters: \n",
    "# \tbootstrap: True\n",
    "# \tcriterion: 'gini'\n",
    "# \tmax_depth: 50\n",
    "# \tmax_features: 'log2'\n",
    "# \tn_estimators: 1000\n",
    "\n",
    "# UNCOMMENT OUT EVERYTHING BELOW TO GO WITH LAST GridSearchCV RESULT\n",
    "best_parameters = {}\n",
    "best_parameters['bootstrap'] = True\n",
    "best_parameters['criterion'] = 'entropy'\n",
    "best_parameters['max_depth'] = 50\n",
    "best_parameters['max_features'] = 'auto'\n",
    "best_parameters['n_estimators'] = 1000\n",
    "# UNCOMMENT OUT EVERYTHING ABOVE TO GO WITH LAST GridSearchCV RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Grid Search found the following optimal parameters: \n\tbootstrap: True\n\tcriterion: 'gini'\n\tmax_depth: 50\n\tmax_features: 'auto'\n\tn_estimators: 1000\n"
    }
   ],
   "source": [
    "print(\"Grid Search found the following optimal parameters: \")\n",
    "for param_name in sorted(best_parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))# # GridSearchCV hyperparameters TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[Parallel(n_jobs=-1)]: Using backend DaskDistributedBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.7s\n[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   28.1s\n[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  1.2min\n[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  2.2min\n[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  2.9min finished\nCPU times: user 2min 14s, sys: 20.9 s, total: 2min 35s\nWall time: 4min 15s\n"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(\n",
    "    oob_score = True,\n",
    "    bootstrap=best_parameters['bootstrap'],\n",
    "    criterion=best_parameters['criterion'],\n",
    "    max_depth=best_parameters['max_depth'],\n",
    "    max_features=best_parameters['max_features'],\n",
    "    n_estimators=best_parameters['n_estimators'],\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "with joblib.parallel_backend('dask', scatter=[data_train, y_train]):\n",
    "    %time clf.fit(data_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Predict</h4>\n",
    "<h5>Training Data</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.2s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    1.2s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    2.8s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    5.2s\n[Parallel(n_jobs=16)]: Done 1000 out of 1000 | elapsed:    6.6s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.2s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    1.2s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    2.8s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    5.1s\n[Parallel(n_jobs=16)]: Done 1000 out of 1000 | elapsed:    6.6s finished\n\nscore:  0.9949074074074075 \n\n\noob score:  0.7973905723905724 \n\n                         precision    recall  f1-score   support\n\n             functional       0.99      1.00      1.00     25802\n         non functional       0.98      0.97      0.97      3466\nfunctional needs repair       1.00      1.00      1.00     18252\n\n               accuracy                           0.99     47520\n              macro avg       0.99      0.99      0.99     47520\n           weighted avg       0.99      0.99      0.99     47520\n\n"
    }
   ],
   "source": [
    "with joblib.parallel_backend('dask', scatter=[data_train, y_train]):\n",
    "    pred_train = clf.predict(data_train)\n",
    "    print(\"\\nscore: \", clf.score(data_train, y_train), \"\\n\")\n",
    "    print(\"\\noob score: \", clf.oob_score_, \"\\n\")\n",
    "    print(classification_report(y_train, pred_train, target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Testing Data</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.2s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    1.0s\n[Parallel(n_jobs=16)]: Done 1000 out of 1000 | elapsed:    1.2s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.7s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    1.3s\n[Parallel(n_jobs=16)]: Done 1000 out of 1000 | elapsed:    1.7s finished\n\nscore:  0.7978956228956229 \n\n                         precision    recall  f1-score   support\n\n             functional       0.81      0.87      0.84      6457\n         non functional       0.51      0.36      0.42       851\nfunctional needs repair       0.82      0.78      0.80      4572\n\n               accuracy                           0.80     11880\n              macro avg       0.71      0.67      0.69     11880\n           weighted avg       0.79      0.80      0.79     11880\n\n"
    }
   ],
   "source": [
    "with joblib.parallel_backend('dask', scatter=[data_train, y_train]):\n",
    "    pred_test = clf.predict(data_test)\n",
    "    print(\"\\nscore: \", clf.score(data_test, y_test), \"\\n\")\n",
    "    print(classification_report(y_test, pred_test, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bitlearnenvconda6dc930ea082b425c82fd8a0b2d571658",
   "display_name": "Python 3.6.10 64-bit ('learn-env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}